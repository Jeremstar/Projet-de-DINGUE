{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900d88c0-369b-4cfd-a42a-9fbbe2bf72e9",
   "metadata": {},
   "source": [
    "# Prédiction de la qualité d'un film à partir de ses caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fbedf3-3c78-484a-863f-649c74d2eb0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Présentation du projet\n",
    "Le but de notre projet est de créer un modèle économétrique pour prédire le succès d'un film à partir de ses caractéristiques, comme sa durée, son genre, son réalisateur, etc. \n",
    "\n",
    "Nous avons décidé d'utiliser les données d'IMDb, un site de notation et de référencement des films. L'avantage de cette plateforme est qu'elle permet aux internautes de noter les films qu'ils ont vu, ce qui sera ce que l'on considère comme la mesure du succès d'un film. Par ailleurs, de nombreuses informations qui nous seront utiles sont présentes sur ce site et sont plus directement accessibles que sur une plateforme comme Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0edf41-7569-4ae4-9951-1c8f3100cf3d",
   "metadata": {},
   "source": [
    "## Commençons par la mise en place du jeu de données\n",
    "Nous importons une base mise à disposition par IMDb, que nous rendons exploitable par quelques opérations élémentaires.\n",
    "Nous ne conservons que les films qui ont 2.000 votes ou plus : cela permet d'une part d'éviter de considérer les films trop peu votés pour que leur note moyenne soit pertinente, et d'autre part d'avoir un jeu de données plus léger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb35ade-9198-4fa8-b7bd-20a596e5348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "movies_rating = pd.read_csv(\"https://datasets.imdbws.com/title.ratings.tsv.gz\")\n",
    "movies_rating_clean = movies_rating[\"tconst\\taverageRating\\tnumVotes\"].str.split(\"\\\\t\", expand=True)\n",
    "movies_rating_clean.columns = ['ID', 'Note_moyenne', 'Nombre_de_votes']\n",
    "movies_rating_clean['Note_moyenne'] = movies_rating_clean['Note_moyenne'].astype(float)\n",
    "movies_rating_clean['Nombre_de_votes'] = movies_rating_clean['Nombre_de_votes'].astype(float)\n",
    "movies_rating_filtré = movies_rating_clean[movies_rating_clean.Nombre_de_votes > 1999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd7f65-5a27-4309-a20c-dfa94d6b2eca",
   "metadata": {},
   "source": [
    "**Nous voilà maintenant en possession d'une première base de travail**\n",
    "\n",
    "Le problème de ce jeu de donné téléchargé, c'est qu'il ne contient que des informations sur les votes des films. Il n'y a aucune mention d'autres caractéristiques dont nous pourrons avoir besoin, comme son cast. Nous avons contacté les services d'IMDb, mais leur API est payant... Nous avons donc choisi de scraper les informations dont nous avons besoin.\n",
    "\n",
    "Cela dit, la base téléchargée va nous être particulièrement utile ! Nous avons à disposition les identifiants de tous les films de la plateforme qui ont recueilli 2000 votes ou plus, et l'URL des pages des films s'écrit à partir de cet identifiant.\n",
    "\n",
    "Après avoir essayé de scraper les éléments en cherchant des chemins d'accès manuellement dans le code HTML, nous avons trouvé dans chaque page un dictionnaire qui comprend les caractéristiques principales des films. Le code suivant permet de recueillir ces données, les traiter pour les rendre exploitables, et les insérer dans un dataframe.\n",
    "\n",
    "Nous avions des ambitions assez importantes quant aux variables à retenir, mais certains éléments étaient intraçables dans la soupe que nous donne BeautifulSoup. Le dictionnaire que nous pouvons scraper ne contient malheureusement pas des données comme le budget, la langue d'origine, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8009b6-b42d-4873-ac47-21b8527575cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On fabrique le squelette du dataframe que l'on va remplir au fur et à mesure du scrap ; on le fusionnera par la suite\n",
    "#avec la database téléchargée plus tôt\n",
    "#On indique le nom des colonnes, qui sont les variables que l'on choisit de conserver\n",
    "#contentRating est la classification d'age, creator est la société de production\n",
    "df = pd.DataFrame(columns=['name', 'alternateName', 'url', 'contentRating', 'datePublished', 'genre', 'actor', 'director', 'creator', 'Origine', 'Budget', 'duration', 'keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfee69a-74b4-4a12-92eb-373a50b851a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from random import seed\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "#C'est une liste des objets inutiles dans le scrap des pages ; je retire aussi le contenu du dataframe qu'on a déjà (les votes)\n",
    "superflu = [\"@context\", \"@type\", \"image\", \"description\", \"review\", \"trailer\", \"aggregateRating\"]\n",
    "#Celui-ci servira à retirer les images des scénaristes, etc\n",
    "superflu2 = ['@type', 'url']\n",
    "\n",
    "session_obj = requests.Session()\n",
    "\n",
    "\n",
    "#On boucle sur chaque film qu'on considère\n",
    "#Le compteur est cosmétique : il sert à nous rassurer sur le fait que tout se passe bien pendant le scrap\n",
    "compteur = 0\n",
    "for ID in movies_rating_filtré['ID'] :\n",
    "  compteur = compteur+1\n",
    "  print(compteur)\n",
    "  try: #On utilise un try except au cas où on aurait un problème sur une page : on ne veut pas que l'exécution s'arrête après des heures de scrap\n",
    "    time.sleep(0.01) #On ajoute un petit délai pour ne pas surcharger le site de requêtes\n",
    "    url_temp = 'https://www.imdb.com/title/'+ID+'/'\n",
    "    response=session_obj.get(url_temp, headers={\"User-Agent\": \"Mozilla/5.0\"}) #On se fait passer pour une session normale ;) \n",
    "    html = response.content\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    #Le bloc est composé de la partie de chaque page qui contient les informations utiles\n",
    "    #On le transforme en dictionnaire\n",
    "    bloc = soup.find(\"script\", type=\"application/ld+json\").string\n",
    "    dictio = json.loads(bloc)\n",
    "    \n",
    "    #On retire dedans ce qui ne nous intéresse pas\n",
    "    for inutile in superflu :\n",
    "      dictio.pop(inutile, None)\n",
    "\n",
    "    #on ajoute une ligne budget illico presto ATTENTION CA NE MARCHE PAS\n",
    "    liste_budg = soup.find_all(\"label\", class_=\"ipc-metadata-list-item__list-content-item\")\n",
    "    if len(liste_budg) >= 3 and '$' in liste_budg[2] :\n",
    "      budget = liste_budg[2].string\n",
    "      if budget == None :\n",
    "        budget = \"Non renseigné\"\n",
    "      else :\n",
    "        budget = \"\".join([elemnt for elemnt in budget if elemnt.isdigit()])\n",
    "      dictio['Budget'] = budget\n",
    "\n",
    "    #L'allure du dictionnaire n'est pas parfaitement satisfaisante, par exemple chaque acteur est associé à une date de naissance,\n",
    "    #à une photo, etc... On ne conserve que le nom des acteurs, et celui des réalisateurs\n",
    "    \n",
    "    if 'actor' in dictio :\n",
    "      for acteur in dictio['actor'] :\n",
    "        for inutile in superflu2 :\n",
    "          acteur.pop(inutile, None)\n",
    "      for indice, nom in enumerate(dictio['actor']) :\n",
    "        dictio['actor'][indice] = nom['name']\n",
    "\n",
    "    if 'director' in dictio :\n",
    "      for directeur in dictio['director'] :\n",
    "        for inutile in superflu2 :\n",
    "          directeur.pop(inutile, None)\n",
    "      for indice, nom in enumerate(dictio['director']) :\n",
    "        dictio['director'][indice] = nom['name']\n",
    "    \n",
    "    if 'creator' in dictio :\n",
    "      for createur in dictio['creator'] :\n",
    "        createur.pop('@type', None)\n",
    "      for indice, url in enumerate(dictio['creator']) :\n",
    "        dictio['creator'][indice] = url['url']\n",
    "\n",
    "    #Pour la société de production c'est un peu compliqué : on n'a qu'une URL\n",
    "    #Ce qui n'est pas grave, puisqu'on peut retrouver son nom en scrapant cet url !\n",
    "    #Mais le temps d'exécution explose si on le fait ; j'inclus donc ce code (complètement fonctionnel)\n",
    "    #mais en pratique il prend trop de temps à tourner\n",
    "    \n",
    "    if 'creator' in dictio :\n",
    "      for index, createur in enumerate(dictio['creator']) :\n",
    "        url_temp = 'https://www.imdb.com'+createur\n",
    "        response=session_obj.get(url_temp, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        html = response.content\n",
    "        soup = bs(html, \"lxml\")\n",
    "        compagnie_soup = soup.find(\"title\")\n",
    "        if compagnie_soup == None :\n",
    "          compagnie = \"Non renseigné\"\n",
    "        else :\n",
    "          compagnie = compagnie_soup.string\n",
    "        compagnie = compagnie[5:-40] #on garde que l'élément important du titre\n",
    "        dictio['creator'][index] = compagnie\n",
    "\n",
    "    #On ajoute au dictionnaire le pays d'origine, que l'on le trouve dans la date de sortie\n",
    "    date_sortie_soup = soup.find(\"a\", class_=\"ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link\", href=\"/title/\"+ID+\"/releaseinfo?ref_=tt_dt_rdat\")\n",
    "    if date_sortie_soup == None :\n",
    "      date_sortieV2 = \"Non renseigné\"\n",
    "      pays = \"Non renseigné\"\n",
    "    else :\n",
    "      date_sortieV2 = date_sortie_soup.string\n",
    "      b1 = date_sortieV2.find('(')\n",
    "      b2 = date_sortieV2.find(')')\n",
    "      pays = date_sortieV2[b1:b2]\n",
    "      pays=pays[1:]\n",
    "    dictio['Origine'] = pays\n",
    "\n",
    "    #On ajoute dans le dataframe la ligne qui correspond au film\n",
    "    df = df.append(dictio, ignore_index=True)\n",
    "  except:\n",
    "    print('Erreur au rang : '+str(compteur))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bea7b0-52fb-4cc6-be3b-5a31a34f7c19",
   "metadata": {},
   "source": [
    "**Et voilà : nous avons construit une base de donnée grâce au scraping**\n",
    "\n",
    "Nous ne conseillons pas de lancer ce code, puisqu'il nous a fallu plus d'une journée pour obtenir le dataframe, sans compter toutes les fois où notre ordinateur a eu des problèmes de connexion et interrompu l'exécution (ce qui nous a coûté au total 3 jours). En revanche, il est possible de le lancer sur quelques valeurs, pour obtenir un échantillon.\n",
    "\n",
    "Nous mettrons à disposition le dataframe complet, pour pouvoir lancer le reste du code sans scraper à nouveau ces quelques 46.000 pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb43d566-d65d-4008-8f9b-5f76d6b8ad57",
   "metadata": {},
   "source": [
    "**La prochaine étape est donc de fusionner les dataframes et de transformer la classe des objets de chaque colonne, pour les rendre exploitables**\n",
    "\n",
    "Le code suivant permet d'importer le dataframe du scrap (*df*) et de le fusionner avec le dataframe téléchargé et traité (*movies_rating_filtré*), en fabriquant une colonne commune pour permettre la jointure. On prend également le soin de rendre les éléments des colonnes *actor* et *director* comme des listes. Ce code fonctionne malgré l'avertissement qu'il renvoie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c67e0f-15b8-4ee9-9d30-a33e0bc77a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2562/2121277114.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  movies_rating_filtré['url']='/title/'+movies_rating_filtré['ID']+'/'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Jeremstar/Succes_de_films-IMDb/main/Database/IMDB_2000votes.csv', \n",
    "                 converters={\"actor\": lambda x: x.strip(\"[]\").split(\", \"), 'director': lambda y : y.strip(\"[]\").split(\", \")}) \n",
    "\n",
    "movies_rating_filtré['url']='/title/'+movies_rating_filtré['ID']+'/'\n",
    "df_fusionné = df.merge(movies_rating_filtré, on='url',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8ae27-db97-4806-a432-79d386a89f2f",
   "metadata": {},
   "source": [
    "On ordonne ensuite les colonnes dans l'ordre que l'on souhaite, on supprime celles qui posent problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd829920-1601-4612-89cb-31711711c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fusionné = df_fusionné.reindex(columns=['ID_y','name','alternateName','url','contentRating','datePublished','genre','actor','director','creator','Origine','Budget','duration','keywords','Note_moyenne','Nombre_de_votes','ID_x'])\t\n",
    "df_fusionné =df_fusionné.drop(['ID_x', 'Budget', 'alternateName', 'creator'],axis=1) #On drop creator, pour les questions de scrap évoquées plus tôt, et budget parce qu'il est \n",
    "df_fusionné.rename(columns={'ID_y':'ID'}, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ee1a8-cbee-43db-836f-bba285d50ff9",
   "metadata": {},
   "source": [
    "Certaines chaînes de caractères ne sont pas lisibles (on a par exemple des codes \"&apos;\" au lieu de véritables apostrophes).\n",
    "La fonction suivante permet de résoudre ces problèmes, à la fois dans les listes et les string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a86d864-1619-4dca-bf3e-e525ff1c6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correcteur(colonne, old, new) :\n",
    "    if type(df_fusionné[colonne][0]) == list :\n",
    "        all_crews = []\n",
    "        for crew in df_fusionné[colonne]:\n",
    "            crew_corrigé = []\n",
    "            if crew != [] :\n",
    "                for individu in crew :\n",
    "                    crew_corrigé.append(individu.replace(old, new))\n",
    "            all_crews.append(crew_corrigé)\n",
    "        df_fusionné[colonne] = all_crews\n",
    "    else :\n",
    "        df_fusionné[colonne]= df_fusionné[colonne].str.replace(old, new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96376142-8a20-4883-a3b9-f97ef8f9c10f",
   "metadata": {},
   "source": [
    "On applique donc cette fonction de correction à toutes les colonnes qui en ont besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4dace5e-ac30-4297-b6c1-6d7e16c1e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "correcteur('actor', '&apos;', \"'\")\n",
    "correcteur('director', '&apos;', \"'\")\n",
    "correcteur('name', '&apos;', \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f34994-c32c-4955-95b6-2637162f1017",
   "metadata": {},
   "source": [
    "Puisque le dataframe du scraping est maintenant importé depuis un csv, tous les éléments des colonnes sont des string. Pour rendre la base de donnée exploitable, il faut changer la nature de certains éléments, ce que permet le bloc suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb21197e-6e45-4c5e-b0c6-5d02a4e1e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.common import flatten\n",
    "\n",
    "def valeurs_possibles (colonne) :\n",
    "    list_nonflat = df_fusionné[colonne]\n",
    "    flat_list = list(flatten(list_nonflat))\n",
    "    liste_valeurs = list(set(flat_list))\n",
    "    return liste_valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d62ceb38-f248-42b6-84f4-f6d838072dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AO',\n",
       " 'PG-13',\n",
       " 'TV-13',\n",
       " 'T',\n",
       " '0+',\n",
       " nan,\n",
       " 'Accord parental',\n",
       " 'Open',\n",
       " 'TV-Y7',\n",
       " 'Tous publics avec avertissement',\n",
       " '-16',\n",
       " '16 avec avertissement',\n",
       " 'E',\n",
       " 'Approved',\n",
       " 'TV-Y7-FV',\n",
       " 'K-A',\n",
       " '10',\n",
       " 'NC-17',\n",
       " 'Unrated',\n",
       " 'TV-G',\n",
       " 'Passed',\n",
       " 'Tous Publics',\n",
       " 'R',\n",
       " 'Tout public',\n",
       " '9+',\n",
       " 'TV-14',\n",
       " 'Not Rated',\n",
       " '18',\n",
       " '16',\n",
       " 'G',\n",
       " 'M',\n",
       " 'Tous Public',\n",
       " '(Banned)',\n",
       " 'TV-PG',\n",
       " 'TV-MA',\n",
       " 'E10+',\n",
       " 'X',\n",
       " 'GP',\n",
       " '-10',\n",
       " 'TV-Y',\n",
       " 'MA-17',\n",
       " 'Public Averti',\n",
       " '6+',\n",
       " '14+',\n",
       " '13',\n",
       " 'PG',\n",
       " '-12',\n",
       " 'Tous publics',\n",
       " '12 avec avertissement',\n",
       " '10 avec avertissement',\n",
       " '12',\n",
       " 'M/PG',\n",
       " '7']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valeurs_possibles('contentRating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2e7c3ac-5dd2-477a-b359-20349649937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2562/534507345.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_fusionné[colonne]= df_fusionné[colonne].str.replace(old, new)\n"
     ]
    }
   ],
   "source": [
    "correcteur('contentRating', 'Tous Public', 'tous publics')\n",
    "correcteur('contentRating', 'Tous Publics', 'tous publics')\n",
    "correcteur('contentRating', 'Tous public', 'tous publics')\n",
    "correcteur('contentRating', 'Tous publics', 'tous publics')\n",
    "correcteur('contentRating', 'Not Rated', 'Not rated') #Absence de certificat\n",
    "correcteur('contentRating', 'Unrated', 'Not rated') #Idem\n",
    "correcteur('contentRating', '-12', '12')\n",
    "correcteur('contentRating', '10 avec avertissement', '10')\n",
    "correcteur('contentRating', '12 avec avertissement', '12')\n",
    "correcteur('contentRating', 'Passed', 'Approved') #Classification d'avant 1968\n",
    "correcteur('contentRating', '14+', '14')\n",
    "correcteur('contentRating', '(Banned)', 'Banned')\n",
    "correcteur('contentRating', '-16', '16')\n",
    "correcteur('contentRating', 'TV-14', '14')\n",
    "correcteur('contentRating', '16 avec avertissement', '16')\n",
    "correcteur('contentRating', '-10', '10')\n",
    "correcteur('contentRating', 'TV-PG', 'Accord parental')\n",
    "correcteur('contentRating', 'PG-13', 'Accord parental')\n",
    "correcteur('contentRating', 'M/PG', 'Accord parental')\n",
    "correcteur('contentRating', 'PG', 'Accord parental')\n",
    "correcteur('contentRating', '0+', 'tous publics')\n",
    "correcteur('contentRating', 'E1Tous Publics+', 'tous publics')\n",
    "correcteur('contentRating', 'E1Tous Public+', 'tous publics')\n",
    "correcteur('contentRating', 'TV-13', '13')\n",
    "correcteur('contentRating', 'R', '18')\n",
    "correcteur('contentRating', 'GP', 'Accord parental')\n",
    "correcteur('contentRating', 'MA-17', '17')\n",
    "correcteur('contentRating', 'X', '18')\n",
    "correcteur('contentRating', 'TV-Y7-FV', '7')\n",
    "correcteur('contentRating', 'TV-Y7', '7')\n",
    "correcteur('contentRating', 'TV-G', 'Tous Public')\n",
    "correcteur('contentRating', 'G', 'Tous Public')\n",
    "correcteur('contentRating', 'Tous Publics', 'tous publics')\n",
    "correcteur('contentRating', 'Tout public', 'tous publics')\n",
    "correcteur('contentRating', '1Tous Public', 'tous publics')\n",
    "correcteur('contentRating', 'Tous Public+', 'tous publics')\n",
    "correcteur('contentRating', 'NC-17', '18')\n",
    "correcteur('contentRating', 'TV-MA', '18')\n",
    "correcteur('contentRating', 'TV-T', 'tous publics')\n",
    "correcteur('contentRating', 'M', '18')\n",
    "correcteur('contentRating', 'E', 'Éducatif')\n",
    "correcteur('contentRating', 'T', 'tous publics')\n",
    "correcteur('contentRating', 'K-A', 'Erreur')\n",
    "correcteur('contentRating', 'AO', 'Erreur')\n",
    "correcteur('contentRating', 'Open', 'Erreur')\n",
    "correcteur('contentRating', 'tous publicsV-Y', 'tous publics')\n",
    "correcteur('contentRating', 'Éducatif1tous publics+', 'tous publics')\n",
    "correcteur('contentRating', 'tous publicss avec avertissement', 'tous publics')\n",
    "correcteur('contentRating', '1tous publics', 'tous publics')\n",
    "correcteur('contentRating', 'tous publicss', 'tous publics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a39517-bfef-4542-acfe-a6e0a9375fdd",
   "metadata": {},
   "source": [
    "Cette procédure est fastidieuse mais il est difficile de faire autrement pour harmoniser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973bfe5-dea5-49ff-ae9f-11873b46b79b",
   "metadata": {},
   "source": [
    "On convertit à présent les mois, pour les numéroter de 1 à 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f85251b-b776-4b70-80ee-a898ea63e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fusionné['month'] = pd.DatetimeIndex(df_fusionné['datePublished']).month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4defab-9aa1-46a2-9c8f-23c5bdc8dee9",
   "metadata": {},
   "source": [
    "On convertit ensuite la durée du film, pour passer par exemple de \"PT1H32M\" au nombre de minutes du film. Par souci de lisibilité, on construit d'abord une fonction de recodage, que l'on applique ensuite à la colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a2be5c0-f922-4bed-aacd-c7b094a71c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recodage_duree(duree) :\n",
    "    if type(duree) == str :\n",
    "        duree = duree[2:]\n",
    "        if 'H' not in duree :\n",
    "            duree_corrigee = int(duree[-3:-1])\n",
    "        elif 'M' not in duree :\n",
    "            duree_corrigee = int(duree[-2:-1])*60\n",
    "        else :\n",
    "            heures = int(duree[0])*60\n",
    "            duree = duree[2:]\n",
    "            minutes = int(duree[-3:-1])\n",
    "            duree_corrigee = heures + minutes\n",
    "    else :\n",
    "        duree_corrigee = \"Non renseigné\"\n",
    "    return duree_corrigee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5778f24-c673-409a-9fee-6096dc10a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fusionné['duration']=df_fusionné['duration'].apply(lambd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e8668-a054-4f24-9d9b-42065f4acac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ddd8d4-e898-4402-ad42-c4c3b77a9ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
