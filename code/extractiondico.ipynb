{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f3611a-01ad-4291-b93d-f423b96ea9b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m     response\u001b[38;5;241m=\u001b[39msession_obj\u001b[38;5;241m.\u001b[39mget(url_temp, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMozilla/5.0\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     62\u001b[0m     html \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m---> 64\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#Ça c'est le dictionnaire complet de tout ce qui est aspirable\u001b[39;00m\n\u001b[1;32m     69\u001b[0m bloc \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/ld+json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstring\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/bs4/__init__.py:248\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "movies_rating = pd.read_csv(\"https://datasets.imdbws.com/title.ratings.tsv.gz\")\n",
    "\n",
    "movies_rating_beau = movies_rating[\"tconst\\taverageRating\\tnumVotes\"].str.split(\"\\\\t\", expand=True)\n",
    "\n",
    "movies_rating_beau.columns = ['ID', 'Note_moyenne', 'Nombre_de_votes']\n",
    "\n",
    "movies_rating_beau['Note_moyenne'] = movies_rating_beau['Note_moyenne'].astype(float)\n",
    "\n",
    "movies_rating_beau['Nombre_de_votes'] = movies_rating_beau['Nombre_de_votes'].astype(float)\n",
    "\n",
    "movies_rating_épuré = movies_rating_beau[movies_rating_beau.Nombre_de_votes > 499]\n",
    "\n",
    "\n",
    "\n",
    "#contentrating c'est classification d'age ; creator c'est la société de production\n",
    "\n",
    "df = pd.DataFrame(columns=['ID', 'name', 'alternateName', 'url', 'contentRating', 'datePublished', 'genre', 'actor', 'director', 'creator', 'Origine', 'duration', 'keywords'])\n",
    "\n",
    "\n",
    "\n",
    "#from pandas.core.dtypes.common import classes_and_not_datetimelike\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import requests\n",
    "\n",
    "from random import seed\n",
    "\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "#liste des objets inutiles dans le scrap des pages ; je retire aussi le contenu du df qu'on a déjà (les votes)\n",
    "\n",
    "superflu = [\"@context\", \"@type\", \"image\", \"description\", \"review\", \"trailer\", \"aggregateRating\"]\n",
    "\n",
    "#pour tej les images des scénaristes etc\n",
    "\n",
    "superflu2 = ['@type', 'url']\n",
    "\n",
    "\n",
    "session_obj = requests.Session()\n",
    "\n",
    "\n",
    "\n",
    "#on boucle sur chaque film qu'on considère\n",
    "\n",
    "for ID in movies_rating_épuré['ID'] :\n",
    "\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    url_temp = 'https://www.imdb.com/title/'+ID+'/'\n",
    "\n",
    "    response=session_obj.get(url_temp, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "    html = response.content\n",
    "\n",
    "    soup = bs(html, \"lxml\")\n",
    "\n",
    "\n",
    "#Ça c'est le dictionnaire complet de tout ce qui est aspirable\n",
    "\n",
    "bloc = soup.find(\"script\", type=\"application/ld+json\").string\n",
    "\n",
    "dictio = json.loads(bloc)\n",
    "\n",
    "\n",
    "#on retire dedans ce qui nous intéresse pas\n",
    "\n",
    "for inutile in superflu :\n",
    "\n",
    "    dictio.pop(inutile, None)\n",
    "\n",
    "\n",
    "\n",
    "#on gère les dictionnaires relou pour garder que le nom des personnes\n",
    " \n",
    "if 'actor' in dictio :\n",
    "    \n",
    "    for acteur in dictio['actor'] :\n",
    "    \n",
    "        for inutile in superflu2 :\n",
    "    \n",
    "            acteur.pop(inutile, None)\n",
    "\n",
    "    for indice, nom in enumerate(dictio['actor']) :\n",
    "    \n",
    "            dictio['actor'][indice] = nom['name']\n",
    "        \n",
    "\n",
    "if 'director' in dictio :\n",
    "\n",
    "    for directeur in dictio['director'] :\n",
    "    \n",
    "        for inutile in superflu2 :\n",
    "            \n",
    "            directeur.pop(inutile, None)\n",
    "            \n",
    "        for indice, nom in enumerate(dictio['director']) :\n",
    "\n",
    "            dictio['director'][indice] = nom['name']\n",
    "            \n",
    "            \n",
    "#pour la société de prod c'est compliqué, ya que une url qui fait ouvrir une page de imdb pro\n",
    "#d'ailleurs le nom de la société c'est une info qu'on peut pas obtenir autrement\n",
    "#je pense qu'on peut trifouiller pour l'avoir sans payer mais là faut se contenter de l'url\n",
    "\n",
    "if 'creator' in dictio :\n",
    "    \n",
    "    for createur in dictio['creator'] :\n",
    "\n",
    "        createur.pop('@type', None)\n",
    "\n",
    "    for indice, url in enumerate(dictio['creator']) :\n",
    "    \n",
    "        dictio['creator'][indice] = url['url']\n",
    "\n",
    "\n",
    "        \n",
    "#on ajoute au dictionnaire le pays d'origine ; on le trouve dans la date de sortie\n",
    "\n",
    "date_sortie_soup = soup.find(\"a\", class_=\"ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link\", href=\"/title/\"+ID+\"/releaseinfo?ref_=tt_dt_rdat\")\n",
    "\n",
    "if date_sortie_soup == None :\n",
    "\n",
    "    date_sortieV2 = \"Non renseigné\"\n",
    "\n",
    "    pays = \"Non renseigné\"\n",
    "\n",
    "else :\n",
    "    \n",
    "    date_sortieV2 = date_sortie_soup.string\n",
    "\n",
    "    b1 = date_sortieV2.find('(')\n",
    "\n",
    "    b2 = date_sortieV2.find(')')\n",
    "\n",
    "    pays = A[b1:b2]\n",
    "\n",
    "    pays=pays[1:]\n",
    "\n",
    "    \n",
    "dictio['Origine'] = pays\n",
    "\n",
    "\n",
    "print(dictio)\n",
    "\n",
    "df = df.append(dictio, ignore_index=True)\n",
    "\n",
    "print(df)\n",
    "\n",
    "#c'est un test de fabrication de dataframe individuel, pour voir si tout marche bien\n",
    "#df_temp = pd.DataFrame(list(dictio.values()), index=dictio.keys()).transpose()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd0160-2331-4dc1-8085-98d0f02c60cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
