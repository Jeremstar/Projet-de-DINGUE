{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900d88c0-369b-4cfd-a42a-9fbbe2bf72e9",
   "metadata": {},
   "source": [
    "# Prédiction de la qualité d'un film à partir de ses caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fbedf3-3c78-484a-863f-649c74d2eb0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Présentation du projet\n",
    "Le but de notre projet est de créer un modèle économétrique pour prédire le succès d'un film à partir de ses caractéristiques, comme sa durée, son genre, son réalisateur, etc. \n",
    "\n",
    "Nous avons décidé d'utiliser les données d'IMDb, un site de notation et de référencement des films. L'avantage de cette plateforme est qu'elle permet aux internautes de noter les films qu'ils ont vu, ce qui sera ce que l'on considère comme la mesure du succès d'un film. Par ailleurs, de nombreuses informations qui nous seront utiles sont présentes sur ce site et sont plus directement accessibles que sur une plateforme comme Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0edf41-7569-4ae4-9951-1c8f3100cf3d",
   "metadata": {},
   "source": [
    "## Commençons par la mise en place du jeu de données\n",
    "Nous importons une base mise à disposition par IMDb, que nous rendons exploitable par quelques opérations élémentaires.\n",
    "Nous ne conservons que les films qui ont 2.000 votes ou plus : cela permet d'une part d'éviter de considérer les films trop peu votés pour que leur note moyenne soit pertinente, et d'autre part d'avoir un jeu de données plus léger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb35ade-9198-4fa8-b7bd-20a596e5348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "movies_rating = pd.read_csv(\"https://datasets.imdbws.com/title.ratings.tsv.gz\")\n",
    "movies_rating_clean = movies_rating[\"tconst\\taverageRating\\tnumVotes\"].str.split(\"\\\\t\", expand=True)\n",
    "movies_rating_clean.columns = ['ID', 'Note_moyenne', 'Nombre_de_votes']\n",
    "movies_rating_clean['Note_moyenne'] = movies_rating_clean['Note_moyenne'].astype(float)\n",
    "movies_rating_clean['Nombre_de_votes'] = movies_rating_clean['Nombre_de_votes'].astype(float)\n",
    "movies_rating_filtré = movies_rating_clean[movies_rating_clean.Nombre_de_votes > 1999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd7f65-5a27-4309-a20c-dfa94d6b2eca",
   "metadata": {},
   "source": [
    "**Nous voilà maintenant en possession d'une première base de travail**\n",
    "\n",
    "Le problème de ce jeu de donné téléchargé, c'est qu'il ne contient que des informations sur les votes des films. Il n'y a aucune mention d'autres caractéristiques dont nous pourrons avoir besoin, comme son cast. Nous avons contacté les services d'IMDb, mais leur API est payant... Nous avons donc choisi de scraper les informations dont nous avons besoin.\n",
    "\n",
    "Cela dit, la base téléchargée va nous être particulièrement utile ! Nous avons à disposition les identifiants de tous les films de la plateforme qui ont recueilli 2000 votes ou plus, et l'URL des pages des films s'écrit à partir de cet identifiant.\n",
    "\n",
    "Après avoir essayé de scraper les éléments en cherchant des chemins d'accès manuellement dans le code HTML, nous avons trouvé dans chaque page un dictionnaire qui comprend les caractéristiques principales des films. Le code suivant permet de recueillir ces données, les traiter pour les rendre exploitables, et les insérer dans un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8009b6-b42d-4873-ac47-21b8527575cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On fabrique le squelette du dataframe que l'on va remplir au fur et à mesure du scrap ; on le fusionnera par la suite\n",
    "#avec la database téléchargée plus tôt\n",
    "#On indique le nom des colonnes, qui sont les variables que l'on choisit de conserver\n",
    "#contentRating est la classification d'age, creator est la société de production\n",
    "df = pd.DataFrame(columns=['name', 'alternateName', 'url', 'contentRating', 'datePublished', 'genre', 'actor', 'director', 'creator', 'Origine', 'Budget', 'duration', 'keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bcba3-2918-46d7-b303-61fb1cdec89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from random import seed\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "#C'est une liste des objets inutiles dans le scrap des pages ; je retire aussi le contenu du dataframe qu'on a déjà (les votes)\n",
    "superflu = [\"@context\", \"@type\", \"image\", \"description\", \"review\", \"trailer\", \"aggregateRating\"]\n",
    "#Celui-ci servira à retirer les images des scénaristes, etc\n",
    "superflu2 = ['@type', 'url']\n",
    "\n",
    "session_obj = requests.Session()\n",
    "\n",
    "\n",
    "#On boucle sur chaque film qu'on considère\n",
    "#Le compteur est cosmétique : il sert à nous rassurer sur le fait que tout se passe bien pendant le scrap\n",
    "compteur = 0\n",
    "for ID in movies_rating_pur['ID'] :\n",
    "  compteur = compteur+1\n",
    "  print(compteur)\n",
    "  try: #On utilise un try except au cas où on aurait un problème sur une page : on ne veut pas que l'exécution s'arrête après des heures de scrap\n",
    "    time.sleep(0.01) #On ajoute un petit délai pour ne pas surcharger le site de requêtes\n",
    "    url_temp = 'https://www.imdb.com/title/'+ID+'/'\n",
    "    response=session_obj.get(url_temp, headers={\"User-Agent\": \"Mozilla/5.0\"}) #On se fait passer pour une session normale ;) \n",
    "    html = response.content\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    #Le bloc est composé de la partie de chaque page qui contient les informations utiles\n",
    "    #On le transforme en dictionnaire\n",
    "    bloc = soup.find(\"script\", type=\"application/ld+json\").string\n",
    "    dictio = json.loads(bloc)\n",
    "    \n",
    "    #On retire dedans ce qui ne nous intéresse pas\n",
    "    for inutile in superflu :\n",
    "      dictio.pop(inutile, None)\n",
    "\n",
    "    #on ajoute une ligne budget illico presto ATTENTION CA NE MARCHE PAS\n",
    "    liste_budg = soup.find_all(\"label\", class_=\"ipc-metadata-list-item__list-content-item\")\n",
    "    if len(liste_budg) >= 3 and '$' in liste_budg[2] :\n",
    "      budget = liste_budg[2].string\n",
    "      if budget == None :\n",
    "        budget = \"Non renseigné\"\n",
    "      else :\n",
    "        budget = \"\".join([elemnt for elemnt in budget if elemnt.isdigit()])\n",
    "      dictio['Budget'] = budget\n",
    "\n",
    "    #L'allure du dictionnaire n'est pas parfaitement satisfaisante, par exemple chaque acteur est associé à une date de naissance,\n",
    "    #à une photo, etc... On ne conserve que le nom des acteurs, et celui des réalisateurs\n",
    "    \n",
    "    if 'actor' in dictio :\n",
    "      for acteur in dictio['actor'] :\n",
    "        for inutile in superflu2 :\n",
    "          acteur.pop(inutile, None)\n",
    "      for indice, nom in enumerate(dictio['actor']) :\n",
    "        dictio['actor'][indice] = nom['name']\n",
    "\n",
    "    if 'director' in dictio :\n",
    "      for directeur in dictio['director'] :\n",
    "        for inutile in superflu2 :\n",
    "          directeur.pop(inutile, None)\n",
    "      for indice, nom in enumerate(dictio['director']) :\n",
    "        dictio['director'][indice] = nom['name']\n",
    "    \n",
    "    if 'creator' in dictio :\n",
    "      for createur in dictio['creator'] :\n",
    "        createur.pop('@type', None)\n",
    "      for indice, url in enumerate(dictio['creator']) :\n",
    "        dictio['creator'][indice] = url['url']\n",
    "\n",
    "    #Pour la société de production c'est un peu compliqué : on n'a qu'une URL\n",
    "    #Ce qui n'est pas grave, puisqu'on peut retrouver son nom en scrapant cet url !\n",
    "    #Mais le temps d'exécution explose si on le fait ; j'inclus donc ce code (complètement fonctionnel)\n",
    "    #mais en pratique il prend trop de temps à tourner\n",
    "    \n",
    "    if 'creator' in dictio :\n",
    "      for index, createur in enumerate(dictio['creator']) :\n",
    "        url_temp = 'https://www.imdb.com'+createur\n",
    "        response=session_obj.get(url_temp, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        html = response.content\n",
    "        soup = bs(html, \"lxml\")\n",
    "        compagnie_soup = soup.find(\"title\")\n",
    "        if compagnie_soup == None :\n",
    "          compagnie = \"Non renseigné\"\n",
    "        else :\n",
    "          compagnie = compagnie_soup.string\n",
    "        compagnie = compagnie[5:-40] #on garde que l'élément important du titre\n",
    "        dictio['creator'][index] = compagnie\n",
    "\n",
    "    #On ajoute au dictionnaire le pays d'origine, que l'on le trouve dans la date de sortie\n",
    "    date_sortie_soup = soup.find(\"a\", class_=\"ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link\", href=\"/title/\"+ID+\"/releaseinfo?ref_=tt_dt_rdat\")\n",
    "    if date_sortie_soup == None :\n",
    "      date_sortieV2 = \"Non renseigné\"\n",
    "      pays = \"Non renseigné\"\n",
    "    else :\n",
    "      date_sortieV2 = date_sortie_soup.string\n",
    "      b1 = date_sortieV2.find('(')\n",
    "      b2 = date_sortieV2.find(')')\n",
    "      pays = date_sortieV2[b1:b2]\n",
    "      pays=pays[1:]\n",
    "    dictio['Origine'] = pays\n",
    "\n",
    "    #On ajoute dans le dataframe la ligne qui correspond au film\n",
    "    df = df.append(dictio, ignore_index=True)\n",
    "    sauvegarde_imdb = df.to_csv('IMDB_2000votes.csv', index = True) #Et par précaution, on sauvegarde le dataframe à chaque itération\n",
    "  except:\n",
    "    print('Erreur au rang : '+str(compteur))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bea7b0-52fb-4cc6-be3b-5a31a34f7c19",
   "metadata": {},
   "source": [
    "**Et voilà : nous avons construit une base de donnée grâce au scraping**\n",
    "\n",
    "Je ne conseille pas de lancer ce code, puisqu'il nous a fallu plus d'une journée pour obtenir le dataframe, sans compter toutes les fois où notre ordinateur a eu des problèmes de connexion et interrompu l'exécution (ce qui nous a coûté au total 3 jours). En revanche, il est possible de le lancer sur quelques valeurs, pour obtenir un échantillon.\n",
    "\n",
    "Nous mettrons à disposition le dataframe complet, pour pouvoir lancer le reste du code sans scraper à nouveau ces quelques 46.000 pages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
